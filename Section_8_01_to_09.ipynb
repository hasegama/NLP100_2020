{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第8章: ニューラルネット  \n",
    "第6章で取り組んだニュース記事のカテゴリ分類を題材として，ニューラルネットワークでカテゴリ分類モデルを実装する．なお，この章ではPyTorch, TensorFlow, Chainerなどの機械学習プラットフォームを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b    5627\n",
       "e    5279\n",
       "t    1524\n",
       "m     910\n",
       "Name: CATEGORY, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 事例の抽出\n",
    "news_corpora = pd.read_csv('./Section_6/NewsAggregatorDataset/newsCorpora.csv',sep='\\t',header=None)\n",
    "news_corpora.columns = ['ID','TITLE','URL','PUBLISHER','CATEGORY','STORY','HOSTNAME','TIMESTAMP']\n",
    "publisher = ['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']\n",
    "ls_is_specified = [news_corpora.PUBLISHER == p for p in publisher]\n",
    "is_specified =reduce(lambda a, b: a | b, ls_is_specified)\n",
    "df = news_corpora[is_specified]\n",
    "#  3. 並び替え\n",
    "df = df.sample(frac=1) # 全てをサンプリングするので、並び替えと等価\n",
    "# 4.保存\n",
    "train_df, valid_test_df = train_test_split(df, test_size=0.2) # 8:2\n",
    "valid_df, test_df = train_test_split(valid_test_df, test_size=0.5) # 8:1:1\n",
    "# train_df.to_csv('./Section_8/train.txt', columns = ['CATEGORY','TITLE'], sep='\\t',header=False, index=False)\n",
    "# valid_df.to_csv('./Section_8/valid.txt', columns = ['CATEGORY','TITLE'], sep='\\t',header=False, index=False)\n",
    "# test_df.to_csv('./Section_8/test.txt', columns = ['CATEGORY','TITLE'], sep='\\t',header=False, index=False)\n",
    "#  事例数の確認\n",
    "df['CATEGORY'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70. 単語ベクトルの和による特徴量  \n",
    "問題50で構築した学習データ，検証データ，評価データを行列・ベクトルに変換したい．例えば，学習データについて，すべての事例xiの特徴ベクトルxiを並べた行列Xと，正解ラベルを並べた行列（ベクトル）Yを作成したい．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train_df.loc[:,['CATEGORY','TITLE']].reset_index()\n",
    "valid=valid_df.loc[:,['CATEGORY','TITLE']].reset_index()\n",
    "test=test_df.loc[:,['CATEGORY','TITLE']].reset_index()\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./Section_7/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "d = {'b':0, 't':1, 'e':2, 'm':3}\n",
    "y_train = train.loc[:,\"CATEGORY\"].replace(d)\n",
    "y_train.to_csv('./Section_8/y_train.txt',header=False, index=False)\n",
    "y_valid = valid.loc[:,\"CATEGORY\"].replace(d)\n",
    "y_valid.to_csv('./Section_8/y_valid.txt',header=False, index=False)\n",
    "y_test = test.loc[:,\"CATEGORY\"].replace(d)\n",
    "y_test.to_csv('./Section_8/y_test.txt',header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    2\n",
       "3    1\n",
       "4    2\n",
       "Name: CATEGORY, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_X(file_name, df):\n",
    "    with open(file_name,'w') as f:\n",
    "        for text in df.loc[:,\"TITLE\"]:\n",
    "            vectors = []\n",
    "            for word in text.split():\n",
    "                if word in model.vocab:\n",
    "                    vectors.append(model[word])\n",
    "            if (len(vectors)==0):\n",
    "                vector = np.zeros(300)\n",
    "            else:\n",
    "                vectors = np.array(vectors)\n",
    "                vector = vectors.mean(axis=0)\n",
    "            vector = vector.astype(np.str).tolist()\n",
    "            output = ' '.join(vector)+'\\n'\n",
    "            f.write(output)\n",
    "write_X('./Section_8/X_train.txt', train)\n",
    "write_X('./Section_8/X_valid.txt', valid)\n",
    "write_X('./Section_8/X_test.txt', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "71. 単層ニューラルネットワークによる予測  \n",
    "問題70で保存した行列を読み込み，学習データについて以下の計算を実行せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.loadtxt(\"./Section_8/X_train.txt\", delimiter=\" \",dtype=np.float32)\n",
    "X_train_tensor = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "\n",
    "X_valid = np.loadtxt(\"./Section_8/X_valid.txt\", delimiter=\" \",dtype=np.float32)\n",
    "X_valid_tensor = tf.data.Dataset.from_tensor_slices(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10672, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10672"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(X_train_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "w_shape=[X_train.shape[1],batch_size]\n",
    "\n",
    "#ReLUではなくsoftmaxだが思い出すためにHeの初期値を利用\n",
    "he_init = tf.cast(tf.sqrt(2./(w_shape[0]*w_shape[1])), dtype=tf.float32)\n",
    "W = tf.Variable(tf.random.truncated_normal(w_shape, stddev=he_init), dtype=tf.float32)\n",
    "b = tf.Variable(tf.zeros([batch_size]), dtype=tf.float32)\n",
    "\n",
    "@tf.function\n",
    "def softmax(X):\n",
    "    return tf.nn.softmax(tf.matmul(X,W) + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for X in X_train_tensor.batch(1).take(1):\n",
    "#     tf.print(softmax(X))\n",
    "# for X in X_train_tensor.batch(4).take(1):\n",
    "#     tf.print(softmax(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "72. 損失と勾配の計算  \n",
    "学習データの事例x1と事例集合x1,x2,x3,x4に対して，クロスエントロピー損失と，行列Wに対する勾配を計算せよ．なお，ある事例xiに対して損失は次式で計算される．  \n",
    "li=−log[事例xiがyiに分類される確率]  \n",
    "ただし，事例集合に対するクロスエントロピー損失は，その集合に含まれる各事例の損失の平均とする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.loadtxt(\"./Section_8/y_train.txt\",dtype=int)\n",
    "y_train_onehot = np.identity(num_class)[y_train]\n",
    "y_train_tensor = tf.data.Dataset.from_tensor_slices(y_train_onehot)\n",
    "\n",
    "y_valid = np.loadtxt(\"./Section_8/y_valid.txt\",dtype=int)\n",
    "y_valid_onehot = np.identity(num_class)[y_valid]\n",
    "y_valid_tensor = tf.data.Dataset.from_tensor_slices(y_valid_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10672,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10672, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (4,), types: tf.float64>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10672"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(y_train_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def crossentropyLoss(X,y):\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(logits=tf.matmul(X,W)+b, labels=y, name=None)\n",
    "#     >>print (crossentropyLoss(X_train[:1],y_train_onehot[:1]))\n",
    "\n",
    "#     return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=tf.matmul(X,W)+b, labels=y, name=None)\n",
    "#     >> print (crossentropyLoss(X_train[:1],y_train[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1.3660189], shape=(1,), dtype=float32)\n",
      "tf.Tensor([1.366019  1.3712292 1.3590018 1.3420788], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print (crossentropyLoss(X_train[:1],y_train_onehot[:1]))\n",
    "print (crossentropyLoss(X_train[:4],y_train_onehot[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3660189, 1.3712293, 1.3590018, 1.3420787]\n"
     ]
    }
   ],
   "source": [
    "# 確認\n",
    "ans=[]\n",
    "for s,i in zip(softmax(X_train[:4]),y_train[:4]):\n",
    "  ans.append(-np.log(s[i]))\n",
    "print (ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "73. 確率的勾配降下法による学習  \n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，行列Wを学習せよ．なお，学習は適当な基準で終了させればよい（例えば「100エポックで終了」など）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://note.nkmk.me/python-tensorflow-keras-basics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer を継承するパターン\n",
    "# class LogisticRegression(tf.keras.layers.Layer):\n",
    "#     def __init__(self):\n",
    "#         super(LogisticRegression, self).__init__()\n",
    "#     def build(self, list_size):#[input_size, layer1_size, layer2_size(output_size)]\n",
    "#         self.list_layer=[]\n",
    "#         # prev_size = list_size[0]\n",
    "#         for next_size in list_size[1:]:\n",
    "#             self.list_layer.append(tf.keras.layers.Dense(next_size))\n",
    "            \n",
    "# #         # layer，parameterを定義\n",
    "# #         self.list_W = []\n",
    "# #         self.list_b = []\n",
    "# #         prev_size = list_size[0]\n",
    "# #         for next_size in list_size[1:]:\n",
    "# #             he_init = tf.cast(tf.sqrt(2./(prev_size*next_size)), dtype=tf.float32)\n",
    "# #             W = tf.Variable(tf.random.truncated_normal([prev_size,next_size], stddev=he_init), dtype=tf.float32)\n",
    "# #             b = tf.Variable(tf.zeros([next_size]), dtype=tf.float32)\n",
    "# #             self.list_W.append(W)\n",
    "# #             self.list_b.append(b)\n",
    "\n",
    "#     def __call__(self, X):\n",
    "#         # layerを接続\n",
    "#         next_input = X\n",
    "#         for layer in self.list_layer[:-1]:\n",
    "#             next_input = tf.nn.relu(\n",
    "#                 layer(next_input)\n",
    "#             )\n",
    "#         next_input = self.list_layer[:-1](next_input)\n",
    "#         return self.list_layer[:-1](next_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(tf.keras.Model):\n",
    "    def __init__(self, list_size):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.list_layer=[]\n",
    "        # prev_size = list_size[0]\n",
    "        for next_size in list_size[1:]:\n",
    "            self.list_layer.append(tf.keras.layers.Dense(next_size))\n",
    "\n",
    "    def __call__(self, X, training=False):# trainingはdroppout用\n",
    "        # layerを接続\n",
    "        next_input = X\n",
    "        for layer in self.list_layer[:-1]:\n",
    "            next_input = tf.nn.relu(\n",
    "                layer(next_input)\n",
    "            )\n",
    "        return self.list_layer[-1](next_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInterface(object):\n",
    "    def __init__(self,model,optimizer,loss_func,acc_metric,acc_func):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_func = loss_func\n",
    "        self.acc_metric = acc_metric\n",
    "        self.acc_func = acc_func\n",
    "#         self.model.compile() #必要ならコンパイルしとく\n",
    "#         model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.valid_loss_list = []\n",
    "        self.valid_acc_list = []\n",
    "\n",
    "    def train(self, epoch, batch_size, interval=2):\n",
    "        for _ in range(epoch):\n",
    "            for X, y in zip(\n",
    "                X_train_tensor.batch(batch_size),\n",
    "                y_train_tensor.batch(batch_size)\n",
    "            ):\n",
    "                loss=self.train_step(X, y)\n",
    "            if _ % interval==0:\n",
    "                print(self.model.list_layer[0].weights[0])\n",
    "                self.train_acc_list.append(self.acc_metric.result())\n",
    "                self.acc_metric_result.reset_states()\n",
    "\n",
    "                pred = self.model(X_valid_tensor.batch(100).take(1), training=False)\n",
    "                self.valid_loss_list.append(\n",
    "                    self.loss_func(\n",
    "                        y_valid_tensor.batch(100).take(1),\n",
    "                        pred\n",
    "                    )\n",
    "                )\n",
    "                self.acc_metric.apdate_state(y_valid_tensor.batch(100).take(1), pred)\n",
    "                self.valid_acc_list.append(self.acc_metric.result())\n",
    "                self.acc_metric_result.reset_states()\n",
    "                    \n",
    "        \n",
    "    @tf.function\n",
    "    def train_step(self, X, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self.model(X, training=True)\n",
    "            loss = self.loss_func(y, pred)\n",
    "        grd = tape.gradient(loss, self.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grd,self.model.trainable_weights))\n",
    "        self.acc_metric.update_state(y, pred)# グラフに追加\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def model_predict(self,X):\n",
    "        return self.model.predict(X)\n",
    "                \n",
    "    def calc_accuracy(self, y, pred, from_logits=False):\n",
    "        return self.acc_func(y, pred, from_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_size = [300, 4]\n",
    "model_interface = ModelInterface(\n",
    "    model = LogisticRegression(list_size),\n",
    "    optimizer = tf.keras.optimizers.Adadelta(),\n",
    "    loss_func = tf.keras.losses.categorical_crossentropy,\n",
    "    acc_metric = tf.keras.metrics.CategoricalCrossentropy(from_logits=False),\n",
    "    acc_func = tf.keras.losses.categorical_crossentropy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'dense_5/kernel:0' shape=(300, 4) dtype=float32, numpy=\n",
      "array([[-0.05689063,  0.0796574 , -0.11627828,  0.04311698],\n",
      "       [-0.00302254,  0.10474101,  0.05542445, -0.0359542 ],\n",
      "       [-0.05717737,  0.06750635,  0.00318012,  0.0460308 ],\n",
      "       ...,\n",
      "       [-0.08387233,  0.00767977, -0.09202517, -0.07229059],\n",
      "       [-0.09180865, -0.00207216,  0.08086109,  0.06985685],\n",
      "       [ 0.03778009,  0.07263648,  0.12457413,  0.12222157]],\n",
      "      dtype=float32)>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ModelInterface' object has no attribute 'acc_metric_result'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-b517c0c4d583>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_interface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-f816a967906f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch, batch_size, interval)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc_metric_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ModelInterface' object has no attribute 'acc_metric_result'"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "model_interface.train(EPOCHS, BATCH_SIZE, interval=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "74. 正解率の計測  \n",
    "問題73で求めた行列を用いて学習データおよび評価データの事例を分類したとき，その正解率をそれぞれ求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正解率はmodel.evaluate を行うために必要なcompileの作法を後から知ったので飛ばす"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_interface.valid_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_interface.valid_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "75. 損失と正解率のプロット  \n",
    "問題73のコードを改変し，各エポックのパラメータ更新が完了するたびに，訓練データでの損失，正解率，検証データでの損失，正解率をグラフにプロットし，学習の進捗状況を確認できるようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboardで確認できる…はず"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "76. チェックポイント  \n",
    "問題75のコードを改変し，各エポックのパラメータ更新が完了するたびに，チェックポイント（学習途中のパラメータ（重み行列など）の値や最適化アルゴリズムの内部状態）をファイルに書き出せ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "77. ミニバッチ化  \n",
    "問題76のコードを改変し，B事例ごとに損失・勾配を計算し，行列Wの値を更新せよ（ミニバッチ化）．Bの値を1,2,4,8,…と変化させながら，1エポックの学習に要する時間を比較せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "78. GPU上での学習  \n",
    "問題77のコードを改変し，GPU上で学習を実行せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "79. 多層ニューラルネットワーク  \n",
    "問題78のコードを改変し，バイアス項の導入や多層化など，ニューラルネットワークの形状を変更しながら，高性能なカテゴリ分類器を構築せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
